<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400,900">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans:300,400,700">

    <link rel="stylesheet" href="latex.css">
    <title>TRITON</title>
</head>

<body>
    <style>
        /* Adding style here because for some reason github.io refuses to modify latex.css (maybe caching issues?)*/
        h1 {
            /* margin-top: 100%; */
            text-align: center;
            font-family: Cambrosia;
        }

        span {
            margin: 0;
        }


        h2 {
             margin-top: 5%;
            text-align: center;
            font-family: Cambrosia;
        }



        h3 {
            margin-top: 5%;
            text-align: center;
            font-family: Cambrosia;
        }



        @font-face {
            font-family: "Cambrosia";
            src: url(./fonts/Cambria.ttf) format("truetype");
        }
    </style>


    <!-- This paragraph: The title -->
    <style>
        @-webkit-keyframes animate-svg-stroke-1{0%{stroke-dashoffset:4088.010986328125px;stroke-dasharray:4088.010986328125px}100%{stroke-dashoffset:0;stroke-dasharray:4088.010986328125px}}@keyframes animate-svg-stroke-1{0%{stroke-dashoffset:4088.010986328125px;stroke-dasharray:4088.010986328125px}100%{stroke-dashoffset:0;stroke-dasharray:4088.010986328125px}}@-webkit-keyframes animate-svg-fill-1{0%{fill:transparent}100%{fill:rgb(0,0,0)}}@keyframes animate-svg-fill-1{0%{fill:transparent}100%{fill:rgb(0,0,0)}}.svg-elem-1{-webkit-animation:animate-svg-stroke-1 2s cubic-bezier(0.47,0,0.745,0.715) 0s both,animate-svg-fill-1 1s cubic-bezier(0.55,0.085,0.68,0.53) 0.8s both;animation:animate-svg-stroke-1 2s cubic-bezier(0.47,0,0.745,0.715) 0s both,animate-svg-fill-1 1s cubic-bezier(0.55,0.085,0.68,0.53) 0.8s both}@-webkit-keyframes animate-svg-stroke-2{0%{stroke-dashoffset:434.2503967285156px;stroke-dasharray:434.2503967285156px}100%{stroke-dashoffset:0;stroke-dasharray:434.2503967285156px}}@keyframes animate-svg-stroke-2{0%{stroke-dashoffset:434.2503967285156px;stroke-dasharray:434.2503967285156px}100%{stroke-dashoffset:0;stroke-dasharray:434.2503967285156px}}@-webkit-keyframes animate-svg-fill-2{0%{fill:transparent}100%{fill:rgb(0,0,0)}}@keyframes animate-svg-fill-2{0%{fill:transparent}100%{fill:rgb(0,0,0)}}.svg-elem-2{-webkit-animation:animate-svg-stroke-2 2s cubic-bezier(0.47,0,0.745,0.715) 0.12s both,animate-svg-fill-2 1s cubic-bezier(0.55,0.085,0.68,0.53) 0.9s both;animation:animate-svg-stroke-2 2s cubic-bezier(0.47,0,0.745,0.715) 0.12s both,animate-svg-fill-2 1s cubic-bezier(0.55,0.085,0.68,0.53) 0.9s both}@-webkit-keyframes animate-svg-stroke-3{0%{stroke-dashoffset:574.4503173828125px;stroke-dasharray:574.4503173828125px}100%{stroke-dashoffset:0;stroke-dasharray:574.4503173828125px}}@keyframes animate-svg-stroke-3{0%{stroke-dashoffset:574.4503173828125px;stroke-dasharray:574.4503173828125px}100%{stroke-dashoffset:0;stroke-dasharray:574.4503173828125px}}@-webkit-keyframes animate-svg-fill-3{0%{fill:transparent}100%{fill:rgb(0,0,0)}}@keyframes animate-svg-fill-3{0%{fill:transparent}100%{fill:rgb(0,0,0)}}.svg-elem-3{-webkit-animation:animate-svg-stroke-3 2s cubic-bezier(0.47,0,0.745,0.715) 0.24s both,animate-svg-fill-3 1s cubic-bezier(0.55,0.085,0.68,0.53) 1s both;animation:animate-svg-stroke-3 2s cubic-bezier(0.47,0,0.745,0.715) 0.24s both,animate-svg-fill-3 1s cubic-bezier(0.55,0.085,0.68,0.53) 1s both}@-webkit-keyframes animate-svg-stroke-4{0%{stroke-dashoffset:271.061279296875px;stroke-dasharray:271.061279296875px}100%{stroke-dashoffset:0;stroke-dasharray:271.061279296875px}}@keyframes animate-svg-stroke-4{0%{stroke-dashoffset:271.061279296875px;stroke-dasharray:271.061279296875px}100%{stroke-dashoffset:0;stroke-dasharray:271.061279296875px}}@-webkit-keyframes animate-svg-fill-4{0%{fill:transparent}100%{fill:rgb(0,0,0)}}@keyframes animate-svg-fill-4{0%{fill:transparent}100%{fill:rgb(0,0,0)}}.svg-elem-4{-webkit-animation:animate-svg-stroke-4 2s cubic-bezier(0.47,0,0.745,0.715) 0.36s both,animate-svg-fill-4 1s cubic-bezier(0.55,0.085,0.68,0.53) 1.1s both;animation:animate-svg-stroke-4 2s cubic-bezier(0.47,0,0.745,0.715) 0.36s both,animate-svg-fill-4 1s cubic-bezier(0.55,0.085,0.68,0.53) 1.1s both}@-webkit-keyframes animate-svg-stroke-5{0%{stroke-dashoffset:434.2484436035156px;stroke-dasharray:434.2484436035156px}100%{stroke-dashoffset:0;stroke-dasharray:434.2484436035156px}}@keyframes animate-svg-stroke-5{0%{stroke-dashoffset:434.2484436035156px;stroke-dasharray:434.2484436035156px}100%{stroke-dashoffset:0;stroke-dasharray:434.2484436035156px}}@-webkit-keyframes animate-svg-fill-5{0%{fill:transparent}100%{fill:rgb(0,0,0)}}@keyframes animate-svg-fill-5{0%{fill:transparent}100%{fill:rgb(0,0,0)}}.svg-elem-5{-webkit-animation:animate-svg-stroke-5 2s cubic-bezier(0.47,0,0.745,0.715) 0.48s both,animate-svg-fill-5 1s cubic-bezier(0.55,0.085,0.68,0.53) 1.2000000000000002s both;animation:animate-svg-stroke-5 2s cubic-bezier(0.47,0,0.745,0.715) 0.48s both,animate-svg-fill-5 1s cubic-bezier(0.55,0.085,0.68,0.53) 1.2000000000000002s both}@-webkit-keyframes animate-svg-stroke-6{0%{stroke-dashoffset:505.80511474609375px;stroke-dasharray:505.80511474609375px}100%{stroke-dashoffset:0;stroke-dasharray:505.80511474609375px}}@keyframes animate-svg-stroke-6{0%{stroke-dashoffset:505.80511474609375px;stroke-dasharray:505.80511474609375px}100%{stroke-dashoffset:0;stroke-dasharray:505.80511474609375px}}@-webkit-keyframes animate-svg-fill-6{0%{fill:transparent}100%{fill:rgb(0,0,0)}}@keyframes animate-svg-fill-6{0%{fill:transparent}100%{fill:rgb(0,0,0)}}.svg-elem-6{-webkit-animation:animate-svg-stroke-6 2s cubic-bezier(0.47,0,0.745,0.715) 0.6s both,animate-svg-fill-6 1s cubic-bezier(0.55,0.085,0.68,0.53) 1.3s both;animation:animate-svg-stroke-6 2s cubic-bezier(0.47,0,0.745,0.715) 0.6s both,animate-svg-fill-6 1s cubic-bezier(0.55,0.085,0.68,0.53) 1.3s both}@-webkit-keyframes animate-svg-stroke-7{0%{stroke-dashoffset:629.5541381835938px;stroke-dasharray:629.5541381835938px}100%{stroke-dashoffset:0;stroke-dasharray:629.5541381835938px}}@keyframes animate-svg-stroke-7{0%{stroke-dashoffset:629.5541381835938px;stroke-dasharray:629.5541381835938px}100%{stroke-dashoffset:0;stroke-dasharray:629.5541381835938px}}@-webkit-keyframes animate-svg-fill-7{0%{fill:transparent}100%{fill:rgb(0,0,0)}}@keyframes animate-svg-fill-7{0%{fill:transparent}100%{fill:rgb(0,0,0)}}.svg-elem-7{-webkit-animation:animate-svg-stroke-7 2s cubic-bezier(0.47,0,0.745,0.715) 0.72s both,animate-svg-fill-7 1s cubic-bezier(0.55,0.085,0.68,0.53) 1.4000000000000001s both;animation:animate-svg-stroke-7 2s cubic-bezier(0.47,0,0.745,0.715) 0.72s both,animate-svg-fill-7 1s cubic-bezier(0.55,0.085,0.68,0.53) 1.4000000000000001s both}
    </style>
    <p style="text-align: center">
        <svg width="10%">
        </svg>
        <!--?xml version="1.0" encoding="UTF-8" standalone="no"?-->
        <svg width="50%" height="100%" viewBox="0 0 656 186" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
            <g transform="matrix(1,0,0,1,-58.8674,-135.874)">
                <g id="trident_001_Fills" transform="matrix(3.09631e-17,0.505666,-0.505666,3.09631e-17,718.926,199.075)">
                    <path d="M135.37,147.573L131.744,102.769L132.038,57.813L137.727,63.738L145.873,69.711L134.279,39.052L125.474,10L116.67,39.052L105.075,69.711L113.222,63.738L118.911,57.813L119.205,102.769L115.579,147.573L112.777,161.302L104.887,186.359L96.416,206.411L86.003,224.224L95.535,222.082L104.9,218.098L110.244,212.59L112.288,205.075L112.296,226.226L109.75,239.668L112.025,263.355L68.305,263.355L60.84,261.844L53.929,258.181L47.769,252.255L42.262,244.196L37.759,234.345L34.741,223.007L33.736,208.007L35.958,190.742L44.453,155.258L47.452,137.043L47.749,118.094L45.045,101.176L41.323,88.821L34.729,73.525L27.855,61.584L29.31,73.885L30.005,91.683L27.854,111.491L23.529,124.855L19.624,132.481L14.835,139.272L23.301,136.041L30.174,130.389L36.447,120.749L35.311,138.918L32.751,154.576L28.805,169.002L16.798,195.882L12.014,208.785L10,221.225L10.123,232.727L13.536,247.87L19.671,261.965L27.677,274.694L37.308,286.201L48.5,296.266L61.125,304.259L73.256,308.997L75.575,305.479L82.417,297.177L92.356,289.431L100.925,286.069L110.572,285.177L111.541,319.042L109.185,319.042L109.185,332.092L105.935,332.092L105.935,358.192L109.185,358.192L109.185,1239.09L105.934,1239.09L105.934,1265.19L109.185,1265.19L109.185,1304.34L141.764,1304.34L141.764,1265.19L145.014,1265.19L145.014,1239.09L141.764,1239.09L141.764,358.192L145.014,358.192L145.014,332.092L141.764,332.092L141.764,319.042L139.408,319.042L140.377,285.177L150.023,286.069L158.593,289.431L168.531,297.177L175.373,305.479L177.692,308.997L189.823,304.259L202.449,296.266L213.641,286.201L223.272,274.694L231.278,261.965L237.413,247.87L240.826,232.727L240.949,221.225L238.935,208.785L234.15,195.882L222.144,169.002L218.198,154.576L215.638,138.918L214.502,120.749L220.775,130.389L227.648,136.041L236.114,139.272L231.325,132.481L227.42,124.855L223.095,111.491L220.944,91.683L221.639,73.885L223.093,61.584L216.22,73.525L209.626,88.821L205.904,101.176L203.199,118.094L203.496,137.043L206.495,155.258L214.99,190.742L217.213,208.007L216.208,223.007L213.19,234.345L208.686,244.196L203.18,252.255L197.02,258.181L190.108,261.844L182.643,263.355L138.924,263.355L141.199,239.668L138.653,226.226L138.66,205.075L140.704,212.59L146.049,218.098L155.414,222.082L164.946,224.224L154.533,206.411L146.062,186.359L138.172,161.302L135.37,147.573Z" style="fill-rule: nonzero; stroke: black; stroke-width: 1.98px;" class="svg-elem-1"></path>
                </g>
                <g transform="matrix(1,0,0,1,-44.2227,57.784)">
                    <path d="M111.976,79.715L188.054,79.715L188.054,104.184L181.163,104.184C180.132,100.153 179.113,96.965 178.105,94.621C177.097,92.278 176.03,90.496 174.905,89.278C173.78,88.059 172.573,87.192 171.284,86.676C169.995,86.16 168.062,85.903 165.484,85.903L156.554,85.903L156.554,157.903C156.554,160.668 156.706,162.871 157.011,164.512C157.316,166.153 157.784,167.453 158.417,168.414C159.05,169.375 159.905,170.125 160.984,170.664C162.062,171.203 163.749,171.731 166.046,172.246L166.046,175.692L133.913,175.692L133.913,172.246C135.32,171.918 136.562,171.567 137.64,171.192C138.718,170.817 139.597,170.36 140.277,169.82C140.956,169.281 141.519,168.59 141.964,167.746C142.409,166.903 142.761,165.754 143.019,164.301C143.277,162.848 143.405,160.715 143.405,157.903L143.405,85.903L134.546,85.903C132.343,85.903 130.527,86.113 129.097,86.535C127.667,86.957 126.343,87.836 125.124,89.172C123.905,90.508 122.792,92.406 121.784,94.867C120.777,97.328 119.827,100.434 118.937,104.184L111.976,104.184L111.976,79.715Z" style="fill-rule: nonzero; stroke: black; stroke-width: 1px;" class="svg-elem-2"></path>
                    <path d="M222.648,133.996L222.648,157.903C222.648,161.278 222.777,163.727 223.034,165.25C223.292,166.774 223.679,167.922 224.195,168.695C224.71,169.469 225.413,170.102 226.304,170.594C227.195,171.086 228.718,171.637 230.874,172.246L230.874,175.692L201.202,175.692L201.202,172.246C204.109,171.496 206.054,170.664 207.038,169.75C208.023,168.836 208.667,167.535 208.972,165.848C209.277,164.16 209.429,161.582 209.429,158.113L209.429,97.293C209.429,94.059 209.312,91.703 209.077,90.227C208.843,88.75 208.456,87.602 207.917,86.781C207.378,85.961 206.663,85.305 205.773,84.813C204.882,84.32 203.359,83.77 201.202,83.16L201.202,79.715L236.007,79.715C246.976,79.715 255.179,81.707 260.616,85.692C266.054,89.676 268.773,95.863 268.773,104.254C268.773,110.676 267.003,116.137 263.464,120.637C259.925,125.137 254.616,128.653 247.538,131.184L247.538,131.746C250.398,132.918 252.847,134.594 254.886,136.774C256.925,138.953 259.187,142.34 261.671,146.934L267.999,158.465C269.827,161.84 271.398,164.418 272.71,166.199C274.023,167.981 275.359,169.34 276.718,170.278C278.077,171.215 279.882,171.871 282.132,172.246L282.132,175.692L262.655,175.692C260.874,173.207 259.187,170.535 257.593,167.676C255.999,164.817 254.429,161.91 252.882,158.957L246.695,147.215C244.82,143.653 243.284,141.028 242.089,139.34C240.894,137.653 239.792,136.469 238.784,135.789C237.777,135.11 236.698,134.641 235.55,134.383C234.402,134.125 232.679,133.996 230.382,133.996L222.648,133.996ZM222.648,86.184L222.648,127.949L230.593,127.949C234.671,127.949 237.976,127.656 240.507,127.07C243.038,126.485 245.382,125.371 247.538,123.731C249.695,122.09 251.417,119.805 252.706,116.875C253.995,113.945 254.64,110.348 254.64,106.082C254.64,101.629 253.878,97.926 252.355,94.973C250.831,92.02 248.523,89.77 245.429,88.223C242.335,86.676 238.163,85.903 232.913,85.903C228.695,85.903 225.273,85.996 222.648,86.184Z" style="fill-rule: nonzero; stroke: black; stroke-width: 1px;" class="svg-elem-3"></path>
                    <path d="M312.085,158.113C312.085,161.254 312.191,163.551 312.402,165.004C312.613,166.457 312.964,167.606 313.456,168.449C313.948,169.293 314.675,169.996 315.636,170.559C316.597,171.121 318.155,171.684 320.312,172.246L320.312,175.692L290.64,175.692L290.64,172.246C293.546,171.496 295.491,170.664 296.476,169.75C297.46,168.836 298.105,167.535 298.409,165.848C298.714,164.16 298.866,161.582 298.866,158.113L298.866,97.293C298.866,94.059 298.749,91.703 298.515,90.227C298.28,88.75 297.894,87.602 297.355,86.781C296.816,85.961 296.101,85.305 295.21,84.813C294.32,84.32 292.796,83.77 290.64,83.16L290.64,79.715L320.312,79.715L320.312,83.16C318.249,83.676 316.761,84.192 315.847,84.707C314.933,85.223 314.206,85.867 313.667,86.641C313.128,87.414 312.73,88.574 312.472,90.121C312.214,91.668 312.085,94.059 312.085,97.293L312.085,158.113Z" style="fill-rule: nonzero; stroke: black; stroke-width: 1px;" class="svg-elem-4"></path>
                    <path d="M333.46,79.715L409.538,79.715L409.538,104.184L402.648,104.184C401.616,100.153 400.597,96.965 399.589,94.621C398.581,92.278 397.515,90.496 396.39,89.278C395.265,88.059 394.058,87.192 392.769,86.676C391.48,86.16 389.546,85.903 386.968,85.903L378.038,85.903L378.038,157.903C378.038,160.668 378.191,162.871 378.495,164.512C378.8,166.153 379.269,167.453 379.902,168.414C380.534,169.375 381.39,170.125 382.468,170.664C383.546,171.203 385.234,171.731 387.53,172.246L387.53,175.692L355.398,175.692L355.398,172.246C356.804,171.918 358.046,171.567 359.124,171.192C360.202,170.817 361.081,170.36 361.761,169.82C362.441,169.281 363.003,168.59 363.448,167.746C363.894,166.903 364.245,165.754 364.503,164.301C364.761,162.848 364.89,160.715 364.89,157.903L364.89,85.903L356.03,85.903C353.827,85.903 352.011,86.113 350.581,86.535C349.152,86.957 347.827,87.836 346.609,89.172C345.39,90.508 344.277,92.406 343.269,94.867C342.261,97.328 341.312,100.434 340.421,104.184L333.46,104.184L333.46,79.715Z" style="fill-rule: nonzero; stroke: black; stroke-width: 1px;" class="svg-elem-5"></path>
                    <path d="M458.195,176.817C452.57,176.817 447.448,175.891 442.831,174.039C438.214,172.188 434.253,169.305 430.948,165.391C427.644,161.477 425.089,156.473 423.284,150.379C421.48,144.285 420.577,136.996 420.577,128.512C420.577,118.059 422.171,109.082 425.359,101.582C428.546,94.082 433.105,88.375 439.034,84.461C444.964,80.547 451.843,78.59 459.671,78.59C468.015,78.59 475.034,80.43 480.73,84.11C486.425,87.789 490.714,93.203 493.597,100.352C496.48,107.5 497.921,116.207 497.921,126.473C497.921,137.442 496.28,146.699 492.999,154.246C489.718,161.793 485.089,167.442 479.113,171.192C473.136,174.942 466.163,176.817 458.195,176.817ZM435.062,126.543C435.062,140.934 437.159,151.879 441.355,159.379C445.55,166.879 451.679,170.629 459.741,170.629C464.851,170.629 469.187,169 472.749,165.742C476.312,162.485 478.984,157.715 480.765,151.434C482.546,145.153 483.437,137.817 483.437,129.426C483.437,119.207 482.394,110.735 480.308,104.008C478.222,97.281 475.374,92.395 471.765,89.348C468.155,86.301 463.96,84.778 459.179,84.778C454.96,84.778 451.339,85.762 448.316,87.731C445.292,89.699 442.784,92.5 440.792,96.133C438.8,99.766 437.347,104.16 436.433,109.317C435.519,114.473 435.062,120.215 435.062,126.543Z" style="fill-rule: nonzero; stroke: black; stroke-width: 1px;" class="svg-elem-6"></path>
                    <path d="M571.257,137.653C572.945,140.465 574.363,142.926 575.511,145.035C576.659,147.145 577.843,149.512 579.062,152.137L579.976,152.137C579.741,149.278 579.53,143.594 579.343,135.086C579.155,126.578 579.062,118.903 579.062,112.059L579.062,97.293C579.062,94.059 578.968,91.692 578.78,90.192C578.593,88.692 578.253,87.543 577.761,86.746C577.269,85.949 576.589,85.305 575.722,84.813C574.855,84.32 573.343,83.77 571.187,83.16L571.187,79.715L596.499,79.715L596.499,83.16C594.39,83.723 592.902,84.238 592.034,84.707C591.167,85.176 590.464,85.785 589.925,86.535C589.386,87.285 588.976,88.41 588.695,89.91C588.413,91.41 588.273,93.871 588.273,97.293L588.273,176.113L579.484,176.113L540.32,110.442C537.882,106.363 536.148,103.387 535.116,101.512C534.085,99.637 533.124,97.785 532.234,95.957L531.038,95.957C531.366,99.567 531.601,106.094 531.741,115.539C531.882,124.985 531.952,134.395 531.952,143.77L531.952,158.113C531.952,161.254 532.058,163.551 532.269,165.004C532.48,166.457 532.831,167.606 533.323,168.449C533.816,169.293 534.542,169.996 535.503,170.559C536.464,171.121 538.023,171.684 540.179,172.246L540.179,175.692L514.515,175.692L514.515,172.246C517.421,171.496 519.366,170.664 520.351,169.75C521.335,168.836 521.98,167.535 522.284,165.848C522.589,164.16 522.741,161.582 522.741,158.113L522.741,97.293C522.741,94.059 522.624,91.703 522.39,90.227C522.155,88.75 521.769,87.602 521.23,86.781C520.691,85.961 519.976,85.305 519.085,84.813C518.195,84.32 516.671,83.77 514.515,83.16L514.515,79.715L536.523,79.715L571.257,137.653Z" style="fill-rule: nonzero; stroke: black; stroke-width: 1px;" class="svg-elem-7"></path>
                </g>
            </g>
        </svg>
    </p>
    <h1>Neural Neural Textures Make Sim2Real Consistent</h1>


    <p class="author">
        <a href="https://ryanndagreat.github.io" target="_blank">Ryan Burgert</a>,
        <a href="https://www3.cs.stonybrook.edu/~jishang" target="_blank">Jinghuan Shang</a>, <a href="https://xxli.me/"
            target="_blank">Xiang Li</a>, <a href="http://michaelryoo.com/" target="_blank">Michael S. Ryoo</a>
    </p>
    <!-- <p class="author">Stony Brook University</p> -->
    <!-- <p class="author">To appear at CoRL 2022</p> -->
    <div style="display:flex; flex-direction: row;">
        <div style="flex:20%;"></div>
        <div style="flex:5%;">
            <img src='stony-brook-university-logo.svg' width="100%" />
        </div>
        <div style="flex:5%;">
            <img src='cropped-CoRL-22-Logo-final-2.svg' width="100%" />
        </div>           
        <!-- <div style="flex:10%;"> -->
            <!-- <img src='QRCode.png' width="100%" /> -->
        <!-- </div>            -->
        <div style="flex:20%;"></div>
    </div>
    <p style="text-align: center">
        <a href="TRITON_Paper.pdf" target="_blank">[Paper]</a>
        <a href="https://github.com/TritonPaper/TRITON" target="_blank">[Code]</a>
        <a href="https://www.youtube.com/watch?v=vec2ILsbWuM" target="_blank">[Video]</a>
    </p>

    <h3>Paper #474<br>Come see us at the last poster session!
    </h3>

    <h2>Abstract</h2>

    We present <b>TRITON</b> (<b>T</b>exture <b>R</b>ecovering <b>I</b>mage <b>T</b>ranslation <b>N</b>etwork): an
    unpaired image translation algorithm that achieves temporal consistency over indefinite timescales by generating neural textures on object surfaces. 
    
    <br/>

    At inference time, TRITON takes in a special
    <span style="letter-spacing:-3pt;color:red">R</span>
    <span style="letter-spacing:-3pt;color:green">G</span>
    <span style="color:blue">B</span>
     image representing a 3d scene (which we call a 
     <span style="letter-spacing:-3pt;color:red">U</span>
     <span style="letter-spacing:-3pt;color:green">V</span>
     <span style="color:blue">L</span>
     image), and outputs a realistic 
     <span style="letter-spacing:-3pt;color:red">R</span>
     <span style="letter-spacing:-3pt;color:green">G</span>
     <span style="color:blue">B</span>
     image that can then be used for downstream tasks, such as training a robot. The input
     <span style="letter-spacing:-3pt;color:red">U</span>
     <span style="letter-spacing:-3pt;color:green">V</span>
     <span style="color:blue">L</span>
    images are simple enough that even the most rudimentary 3d renderer can generate them.

    <br/>
    <br/>
    <div>
        <div style="display:flex; flex-direction: row;">
            <!-- <div style="padding-top:30.5%;" name="video_comparer" id="video-compare-container">
                <video loop muted playsinline disablePictureInPicture>
                    <source src="large_assets/robo5_cam_out.mp4">
                </video>
                <div id="video-clipper">
                    <video loop muted playsinline disablePictureInPicture>
                        <source src="large_assets/robo5_cam_uvl.mp4">
                    </video>
                </div>
            </div> -->
            <div style="padding-top:30.5%;" name="video_comparer" id="video-compare-container">
                <video loop muted playsinline disablePictureInPicture>
                    <source src="large_assets/threeduck.mp4">
                </video>
                <div id="video-clipper">
                    <video loop muted playsinline disablePictureInPicture>
                        <source src="large_assets/threeduck_uvl.mp4">
                    </video>
                </div>
            </div>
            <div style="flex:60%; padding-top:30.5%;" name="video_comparer" id="video-compare-container">
                <video loop muted playsinline disablePictureInPicture>
                    <source src="large_assets/fiveItems_cam_anim_1_out.mp4">
                </video>
                <div id="video-clipper">
                    <video loop muted playsinline disablePictureInPicture>
                        <source src="large_assets/fiveItems_cam_anim_1_UVL.mp4">
                    </video>
                </div>
            </div>
        </div>
        <div class="caption" style="text-align: center">
            <i>Move your cursor along the video to move the the divider!<br> On the left is the 
                <span style="letter-spacing:-3pt;color:red">U</span>
                <span style="letter-spacing:-3pt;color:green">V</span>
                <span style="color:blue">L</span>
                input image, and on the right is the translated
                <span style="letter-spacing:-3pt;color:red">R</span>
                <span style="letter-spacing:-3pt;color:green">G</span>
                <span style="color:blue">B</span>
                output image. Note that all <b>training</b> photographs were taken from the <b>same</b> camera position; this video extrapolates new camera positions as well as new object positions.
                This should be animated - if it's not working, please try a Chromium based browser.
            </i> 
        </div>
        <br>
        <div style="display:flex; flex-direction: row;">
            <img width="50%" height="50%" style="box-shadow: 0 3px 3px rgba(0,0,0,0.2);" src="./RobotFiveTextures.svg"/>
            <span style="color:white">||</span> <!-- Spacing -->
            <img width="50%" height="50%" style="box-shadow: 0 3px 3px rgba(0,0,0,0.2);" src="./fiveItemsTextures.svg"/>
        </div>
        <div class="caption" style="text-align: center">
            <i>Two sets of recovered textures, corresponding to the above two videos.
            </i> 
        </div>
        
        <br/>
        <br/>

        <div style="display:flex; flex-direction: row;">
            <div style="flex:10%;"></div>
            <!-- <div style="flex:30%; padding-top:26%;" name="video_comparer" id="video-compare-container"> -->
                <div style="flex:30%; padding-top:26%;" name="video_comparer" id="video-compare-container">
                <video loop muted playsinline disablePictureInPicture>
                    <source src="large_assets/roboarm_plain.mp4">
                </video>
                <div id="video-clipper">
                    <video loop muted playsinline disablePictureInPicture>
                        <source src="large_assets/roboarm_plain_uvl.mp4">
                    </video>
                </div>
            </div>
            <!-- <div style="flex:30%; padding-top:26%;" name="video_comparer" id="video-compare-container"> -->
                <div style="flex:30%; padding-top:26%;" name="video_comparer" id="video-compare-container">
                <video loop muted playsinline disablePictureInPicture>
                    <source src="large_assets/roboarm_decals.mp4">
                </video>
                <div id="video-clipper">
                    <video loop muted playsinline disablePictureInPicture>
                        <source src="large_assets/roboarm_plain_uvl.mp4">
                    </video>
                </div> 
            </div>           
            <div style="flex:10%;"></div>
        </div>
        <div class="caption" style="text-align: center">
            <i>Here's an animation of a robot arm.
            </i> 
        </div>

    </div>
    

    <!-- TRITON can produce photorealistic results given a  -->
    <!-- Given a small set of unlabeled photographs and a set of simulated images,  -->
    <!-- As a black box, TRITON takes in an image and outputs an image.  -->
    
    <h2>How it Works</h2>
    <h3>The Training Data</h3>
    <!-- which takes the UV map and object labels of a 3d scene and renders a realistic -->
    <!-- image. <b>TRITON</b> combines differentiable rendering with image translation to achieve temporal consistency over -->
    <!-- indefinite timescales, using surface consistency losses and <a href="#texture"><i>neural neural textures</i></a>. -->
    To train TRITON, you need an <b>unpaired</b> set of photographs and a set of simulated scenes, rendered as
    <span style="letter-spacing:-3pt;color:red">U</span>
    <span style="letter-spacing:-3pt;color:green">V</span>
    <span style="color:blue">L</span>
    images. <br><br>
    
    <img src='photo_film.svg' />
    <div class="caption" style="text-align: center">
        <i> We need a set of about 100 or so <span style="letter-spacing:-3pt;color:red">R</span>
            <span style="letter-spacing:-3pt;color:green">G</span>
            <span style="color:blue">B</span> real-life photographs
        </i>
    </div>
    
    <br>
    
    <img src='uvl_explanation_minimal.svg' />
    <div class="caption" style="text-align: center">
        <i>
            We also need a set of simulated
            <span style="letter-spacing:-3pt;color:red">U</span>
            <span style="letter-spacing:-3pt;color:green">V</span>
            <span style="color:blue">L</span> images, which encode the
            <span style="letter-spacing:-3pt;color:red">U</span>
            <span style="color:green">V</span>
            coordinates of each object in a scene, as well as the object label <span style="color:blue">L</span>. Because these images can be obtained cheaply, we can use thousands of them.
        </i>
    </div>

    <h3>Neural Neural Textures</h3>
    
    What sets TRITON apart from other image translation algorithms is its use of neural neural textures. 
    Previous works called these learnable textures "neural textures", and were parametrized by a discrete grid of
    differentiable texels. In contrast, we call our learnable textures as <i>neural nerual textures</i>, because our
    textures themselves are represented as a neural network function, parameterized continuously over UV space.
    Using this representation instead of using discrete texels allows TRITON to learn faster and yields better
    results. 
    <img src="oversimplified_texture2.svg" style="" width="75%" class="center"/>
    <div class="caption" style="text-align: center">
        <i> Each 3d object gets its own neural neural texture, which is represented continuously with an MLP.
        </i>
    </div>


    <h3>The General Pipeline</h3>

    <img src="oversimplified_pipeline2.svg" style="" width="90%" class="center"/>
    <div class="caption" style="text-align: center">
        <i> This is a simplified version of the TRITON pipeline. It omits GAN losses as well as surface consistency losses. For more information, as well as a more detailed diagram, please read the paper!
        </i>
    </div>





    
    <h2>Results</h2>

    <h3>Comparison to other Image Translators</h3>
    <div class="video_wrapper">
        <iframe src="https://www.youtube-nocookie.com/embed/JXNLwWoAlgQ?autoplay=1&rel=0" frameborder="0" allowfullscreen></iframe>
    </div>
    <div class="caption"><i>In the above video, we compare the outputs of TRITON to various other image translation
            algorithms. TRITON provides higher quality, temporally consistent results. This is because TRITON is better at
            making use of 3d geometry. or more comparison videos like this, please see our paper's appendix.</i></div>















    <!-- <img src="figure0.png" />
    <br>
    We propose <b>TRITON</b> (<b>T</b>exture <b>R</b>ecovering <b>I</b>mage <b>T</b>ranslation <b>N</b>etwork): an
    unpaired image translation algorithm which takes the UV map and object labels of a 3d scene and renders a realistic
    image. <b>TRITON</b> combines differentiable rendering with image translation to achieve temporal consistency over
    indefinite timescales, using surface consistency losses and <a href="#texture"><i>neural neural textures</i></a>.
    <br>
    <br>
    <p><b>Checkout the videos below and see how TRITON works!</b></p>



    <h2 class="abstract">Sim2Real</h2>


    <br>
    <br>
    <h4>Apply textures to objects from sim</h4>
    <div class="caption">TRITON makes simulated images realistic, while being more consistent than other image
        translation algorithms. On the top row of the video we have input images, and on the bottom row we have TRITON's
        output images. None of these object placements were seen in real life. Note how the surfaces of the objects
        appear consistent throughout the video, even though the cubes have slight shadows under them and the apple and
        soda cans remain shiny. </div>

    <div class="video_wrapper">
        <iframe src="https://www.youtube-nocookie.com/embed/0t0xiVS_8D0?autoplay=1&rel=0" frameborder="0" allowfullscreen ></iframe>
    </div>


    <br />
    <br /> -->

    <!-- <h4>Deformable object</h4>
    <div class="caption">TRITON also works on deformable objects like this American flag.</div>

    <div class="video_wrapper">
        <iframe src="https://www.youtube-nocookie.com/embed/HPk4PHLaut4?autoplay=1&rel=0" frameborder="0" allowfullscreen ></iframe>
    </div> -->

    <!-- <br />
    <br />
    <h4>Comparison with other methods</h4>
    <div class="caption"> We compare TRITON to other image translation algorithms by moving the objects around. Note how
        although each individual frame might look realistic, CycleGAN and CUT let the top of each cube can randomly
        shift whereas they remain the same using TRITON.</div>


    <div class="video_wrapper">
        <iframe src="https://www.youtube-nocookie.com/embed/JXNLwWoAlgQ?autoplay=1&rel=0" frameborder="0" allowfullscreen ></iframe>
    </div>


    <h2 class="abstract">TRITON recovered textures</h2>
    <div class="caption">From TRITON's outputs, we can recover textures for each object. In this image we show the three
        recovered texture sets with respect to the above video.</div>
    <img src="RecoveredTextures.png">

    <br/>
    <img src="RobotFiveRecoveredTextures.png" width="40%" class="center">

    <br />
    <br /> -->
    <!-- <div class="caption">Moreover, TRITON is also able to apply any learned texture to any other objects by giving the
        texture label to that object.</div>

    <div class="video_wrapper">
        <iframe src="https://www.youtube-nocookie.com/embed/ivfBslj4Gsg?autoplay=1&rel=0" frameborder="0" allowfullscreen ></iframe>
    </div> -->


    <!-- Additionally, TRITON can synthesize new viewpoints - even if it was only trained from a single viewpoint. This is because it is able to effectively leverage the underlying geometry from the input images.

    <div class="video_wrapper">
        <iframe src="https://www.youtube-nocookie.com/embed/qlfMPG1_AoU?autoplay=1&rel=0" frameborder="0" allowfullscreen ></iframe>
    </div> -->

    <!-- <div class="caption">We compare TRITON to other image translation algorithms using the moving objects. Note how the top of each cube can randomly shift when using CycleGAN and CUT, while they remain the same using TRITON.</div> -->
    <!-- <br> -->

    <!-- <h2 class="abstract">Methodology</h2>
    <p>TRITON's goal is to turn 3D simulated images into realistic fake photographs (by training it without any matching
        pairs), while maintaining high surface consistency. It does this by simultaneously learning both an image
        translator and a set of realistic textures. TRITON introduces a learnable neural neural texture with two novel
        surface consistency losses to an existing image translator.</p>
    <p>Check the figure below to see how we apply each loss and please refer to our <a href="TRITON_Paper.pdf"
            target="_blank">paper</a> to find more details.</p>
    <p>The code will be released soon!</p>
    <img src="main_diagram_beautiful.png" />
    <div class="caption"></div> -->

    <!-- <h3 id="texture">Neural Neural Textures</h3> -->
    <!-- <p>Previous works called these learnable textures "neural textures", and were parametrized by a discrete grid of
        differentiable texels. In contrast, we call our learnable textures as <i>neural nerual textures</i>, because our
        textures themselves are represented as a neural network function, parameterized continuously over UV space.
        Using this representation instead of using discrete texels allows TRITON to learn faster and yields better
        results.</p>
    <img src="learnable_textures.png" /> -->


    <br>

    <h3>Robot policy trained by sim2real</h3>
    <div class="caption">TRITON enables a robot reacher task. In this sim2real experiment, we train a behavioral cloning
        policy that takes single RGB image from a fixed camera in the simulator and deploy it directly to the real robot
        without further fine-tuning. The action policy predicts the location of all the target objects simultaneously
        and is trained fully by only 2000 photorealistic images generated from TRITON. Check out the demo video below.
    </div>

    <div class="video_wrapper">
        <iframe src="https://www.youtube-nocookie.com/embed/IwAs420hY6A?autoplay=1&rel=0" frameborder="0" allowfullscreen ></iframe>
    </div>

</body>


    <!-- This paragraph: before/after videos -->
    <style>
        #video-compare-container {
            /* flex:30%; */
            padding-top:40.5%;
            /*  */
            display: inline-block;
            line-height: 0;
            position: relative;
            width: 100%;
            height:100%;
            margin: .5%;
            box-shadow: 0 3px 3px rgba(0,0,0,0.2);
            /* border-style: dashed; */
            
        }
        #video-compare-container>video {
            width: 100%;
            position: absolute;
            top: 0;
            height: 100%;
            margin: 0%;
        }
        #video-clipper {
            width: 50%;
            position: absolute;
            top: 0;
            bottom: 0;
            overflow: hidden;
            margin: 0;
        }
        #video-clipper video {
            width: 200%;
            position: absolute;
            height: 100%;
            margin: 0%;
        }
    </style>

    <script>
        // Take all div's with name="video_comparer" and turn them into a side-by-side comparison
        function hookUpComparer(videoContainer)
        {
            let videoClipper = videoContainer.children[1];
            let clippedVideo = videoClipper.getElementsByTagName("video")[0];
            function trackLocation(e) {
                let rect = videoContainer.getBoundingClientRect(),
                    position = ((e.pageX - rect.left) / videoContainer.offsetWidth) * 100;
                if (position <= 100) {
                    videoClipper.style.width = position + "%";
                    clippedVideo.style.width = ((100 / position) * 100) + "%";
                    clippedVideo.style.zIndex = 3;
                }
            }
            videoContainer.addEventListener("mousemove", trackLocation, false);
            videoContainer.addEventListener("touchstart", trackLocation, false);
            videoContainer.addEventListener("touchmove", trackLocation, false);
            console.log(videoContainer)


            function delay(ms) {
                return new Promise(resolve => setTimeout(resolve, ms));
            }
            
            
            function play()
            {
                videoContainer.children[0].muted=true
                videoClipper  .children[0].muted=true
                function do_play()
                {
                    videoContainer.children[0].play()
                    videoClipper  .children[0].play()
                }
                delay(2000).then(do_play); //Make sure the videos are in sync; wait for the page to load
            }

            // delay(10000).then(play); //Make sure the videos are in sync; wait for the page to load
	    

	    // JS trying to fix async videos
            //window.addEventListener('load',play);
	    document.addEventListener("DOMContentLoaded", play);
        }
        for (let video_comparer of document.getElementsByName("video_comparer")) {
            hookUpComparer(video_comparer)
        }
    </script> 

<h2>Citation</h2>

If you would like to cite us, please use the below bibtex citation:

<div style="font-family:monospace;background-color: rgb(232, 232, 232);padding: 10pt;border-radius: 10pt;box-shadow: 0 3px 3px rgba(0,0,0,0.2);">
        @inproceedings{Burgert2022,
        <br/>    author = {Burgert, Ryan and Shang, Jinghuan and Li, Xiang and Ryoo, Michael},
        <br/>    title = {Neural Neural Textures Make Sim2Real Consistent},
        <br/>    booktitle = {Proceedings of the 6th Conference on Robot Learning},
        <br/>    year = {2022},
        <br/>    url = {https://tritonpaper.github.io}
        <br/>    }
      

</div>


</html>

<!-- <iframe src="https://www.youtube-nocookie.com/embed/kecK_cJgLT8?autoplay=1&rel=0" frameborder="0" allowfullscreen ></iframe>  ORIGINAL ROBOT COMPARISON VIDEO-->
    <!-- <iframe src="https://www.youtube-nocookie.com/embed/-GSixT4shxY?autoplay=1&rel=0" frameborder="0" allowfullscreen ></iframe>  BORING COMPARISION WITH JUST CUBES-->


    <!-- 
    In the two below videos, we train on synthetic data so we can quantitatively measure the accuracy of our results. 
NOBODY CARES

    <div class="video_wrapper">
        <iframe src="https://www.youtube-nocookie.com/embed/p6Mt0l8OPvw?autoplay=1&rel=0" frameborder="0" allowfullscreen ></iframe>
    </div>

    <div class="video_wrapper">
        <iframe src="https://www.youtube-nocookie.com/embed/HPk4PHLaut4?autoplay=1&rel=0" frameborder="0" allowfullscreen ></iframe>
    </div>

    <br /> -->
